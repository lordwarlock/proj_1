<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
"http://www.w3.org/TR/html4/strict.dtd">

<html>
  <head>
    <link rev="made" href="mailto:taku@chasen.org">
    <title>YamCha: Yet Another Multipurpose CHunk Annotator</title>
    <link type="text/css" rel="stylesheet" href="yamcha.css">
  </head>

  <body>
    <h1>YamCha: Yet Another Multipurpose CHunk Annotator</h1>

    <p>$Id: index.html,v 1.36 2005/09/05 14:51:34 taku Exp $;</p>

    <h2>Introduction</h2>
      <p><b>YamCha</b> is a generic, customizable, and open source
      text chunker oriented toward a lot of NLP tasks, such as POS
      tagging, Named Entity Recognition, base NP chunking, and Text
      Chunking. <b>YamCha</b> is using a state-of-the-art machine
      learning algorithm called Support Vector Machines (SVMs),
      first introduced by Vapnik in 1995.</p>

      <p><b>YamCha</b> is exactly the same system which performed
      the best in the <a href=
      "http://lcg-www.uia.ac.be/conll2000/chunking/">CoNLL2000
      Shared Task, Chunking</a> and <a href=
      "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">BaseNP
      Chunking task</a>.</p>
    
    <h2>Table of contents</h2>
    <ul>
      <li><a href="#features">Features</a></li>

      <li><a href="#news">News</a></li>

      <li>
        <a href="#download">Download</a> 

        <ul>
          <li><a href="#source">Source</a></li>

          <li><a href="#rpm">Binary/Source package for RedHat
          Linux</a></li>

          <li><a href="#windows">Binary package for
          MS-Windows</a></li>
        </ul>
      </li>

      <li><a href="#install">Installation</a></li>

      <li>
        <a href="#usage">Usage</a> 

        <ul>
          <li><a href="#format">Training and Test file
          formats</a></li>

          <li><a href="#training">Training and Testing</a></li>

          <li><a href="#tuning">Parameter Tuning</a></li>

          <li><a href="#fast">Enable Fast Chunking </a></li>

          <li><a href="#others">Other options</a></li>
        </ul>
      </li>

      <li><a href="#bib">Bibliography</a></li>

      <li><a href="#ack">Acknowledgments</a></li>

      <li><a href="#links">Links</a></li>
    </ul>

    <h2><a name="features">Features</a></h2>

    <ul>
      <li>Moderately high performance chunker based on Support
      Vector Machines</li>

      <li>Independent from the given task, training/testing with
      any data which can be seen as a "generic" text chunking
      task</li>

      <li>Use <a
      href="http://cl.aist-nara.ac.jp/~taku-ku/publications/acl2003.pdf">PKE/
      PKI</a>, whcih make the classification
      (chunking) speed faster than the original SVMs. For details, please see <a
      href="#fast">here</a>.
      <li>Can redefine feature sets (window-size),
      parsing-direction (forward/backward) and algorithms of
      multi-class problem (pair wise/one vs rest)</li>

      <li>Practical chunking time (1 or 2 sec./sentence. it highly
      depends on the task)</li>

      <li>Can perform partial chunking</li>

      <li>C/C++ library</li>
    </ul>

    <h2><a name="news">News</a></h2>
    <ul>
      <li>
     <strong>2005-09-05</strong>: <a href="#download">yamcha 0.33</a>
     Released<br>
     <ul>
      <li>Fix bugs<br>
      <li>Support bag-of-words feature (experimental)<br>
      <li>Support 64 bit machine (experimental)<br>
     </ul>
     <strong>2004-11-29</strong>: yamcha 0.32<br>
       <ul>
        <li>Fix FATAL bugs in feature_index.cpp<br>
            Multiple feature templates (e.g, F:-2..-1:4..15 F:1..2:4..15
            F:0..0:4..) didn't work in the previous releases.<br>
            Mr. Sameer Pradhan gave me a detailed report on this  bug.
        <li>Support NULL '\0' string in the parameters.
            Mr. Kazuma Takaoka. gave me a patch to fix the bug.
       </ul>

     <li><strong>2004-10-03</strong>: yamcha 0.31  Released<br>
       <ul>
        <li>Fix bugs in mkmodel.<br>
	    mkmodel did not work well with Perl 5.8.1 or higer version. 
	<li>Perl/Python/Ruby modules are ready (experimental release)
       </ul>
     <li><strong>2004-03-28</strong>: yamcha 0.30 Released<br>
       <ul>
         <li>Change file formats of binary models.<br>
	     The compatibility of binary models is broken. You need to
	     recompile model files as follows:
<pre>
% yamcha-mkmodel foo.txtmodel.gz foo.model
</pre>
where foo.txtmodel.gz is a text model file generated with the option, MODEL=foo.
Text model files will be found in your working directories.
         <li>Support <a
	     href="http://cl.aist-nara.ac.jp/~taku-ku/publications/acl2003.pdf">PKE (Polynomial Kernel Extended)</a> which makes
	     chunking (classification) speed significantly faster than the original yamcha. <br>
	     To enable PKE, create model files with -e option.
<pre>
% yamcha-mkmodel -e foo.txtmodel.gz foo.model
</pre>
	     Note that PKE is an approximation of the original SVMs.
	     <br> Please see <a href="#fast">here</a> for details. 
	<li>C API is ready.
       </ul>
     <li><strong>2003-08-15</strong>: yamcha 0.27 Released<br>
       <ul>
         <li>Update <a href="../darts">darts</a> library</li>
       </ul>
     <li><strong>2003-07-04</strong>: yamcha 0.26 Released<br>
        <ul>
         <li>Fix the padding problem (bus error) arising in SPARC processor
        </ul>

        <strong>2003-04-06</strong>: yamcha 0.25 Released<br>
        <ul>
          <li>Fix FATAL error on compiling ONE-VS-REST model file. 
	      With this bug, YamCha would output an internal reserved
	      tag (__OTHER__) instead of correct answer tag. <br>
	      If you have *.txtmodel.gz files learned with ONE-VS-REST
	      mode, please try the following command to re-create a correct
	      model file
<pre>
% /usr/local/libexec/yamcha foo.txtmodel.gz foo.model
</pre>
	  </li>
        </ul>
      
     <li><strong>2003-03-29</strong>: yamcha 0.24 Released<br>
     <ul>
          <li>Fix inefficiencies of mkmodel and mkdarts</li> 
     </ul>

      <li>
        <strong>2003-01-31</strong>: yamcha 0.23 Released<br>
        <ul>
          <li>Fix memory leak bugs</li>
	  <li>Use Visual Studio .NET instead of MinGW to build Windows binary
        </ul>
      </li>
    
      <li>
        <strong>2003-01-06</strong>: yamcha
        0.22 Released<br>

        <ul>
          <li>Update param.cpp, yet another command line
          parser</li>
        </ul>
      </li>

      <li>
        <strong>2003-01-01</strong>: yamcha 0.21 Released<br>
        <ul>
         <li>Update <a href="../darts">darts</a> library</li>
        </ul>
      </li>

      <li>
        <strong>2002-11-11</strong>: yamcha 0.2 Released<br>

        <ul>
          <li>Modify many old-fashioned codes</li>

          <li>Can select strategies for multi-class problem:
          pair-wise or one vs rest</li>

          <li>Save memory when a large model file is generated</li>

          <li>Fix bug about column_size parameter</li>

          <li>Use mmap(3) to make the time for initialization
          faster</li>

          <li>Supports gcc 3.2, Borland C++, and Visual Studio
          .NET</li>

          <li>Change the default sentence boundary marker from
          "EOS" to empty.</li>

          <li>Delete Perl/Ruby modules (I will rewrite them using
          <a href="http://www.swig.org">SWIG</a>)</li>

          <li>Delete -f option, use -V option insted.</li>
        </ul>
      </li>

      <li>
        <strong>2001-7-09</strong>: yamcha 0.1<br>

        <ul>
          <li>Initial Release!</li>
        </ul>
      </li>
     </ul>

    <h2><a name="download">Download</a></h2>

    <ul>
      <li><b>YamCha</b> is free software; you can redistribute it
      and/or modify it under the terms of the <a href=
      "http://www.gnu.org/copyleft/lesser.html">GNU Lesser General
      Public License</a>.</li>

      <li><b>YamCha</b> is distributed in the hope that it will be
      useful, but WITHOUT ANY WARRANTY; without even the implied
      warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
      PURPOSE. See <a href=
      "http://www.gnu.org/copyleft/lesser.html">GNU Lesser General
      Public License</a>. the for more details.</li>

      <li>
        Please let <a href=
        "mailto:taku@chasen.org">me</a> know if you use
        <b>YamCha</b> for research purpose or find any research
        publication where <b>YamCha</b> is applied. 

        <h3><a name="source">Source</a></h3>

        <ul>
          <li>yamcha-0.33.tar.gz: <a href=
          "./src/yamcha-0.33.tar.gz">HTTP</a></li>
        </ul>

<!--        <h3><a name="rpm">Binary/Source package for RedHat
        Linux</a></h3>

        <ul>
          <li>RedHat 9.x i386: <a href=
          "./redhat-9.x/RPMS/i386/">HTTP</a></li>

          <li>
            RedHat 9.x SRPMS: <a href=
            "./redhat-9.x/SRPMS/">HTTP</a> 

            <p>Binary packages are built on RedHat 9.0. You must
            re-rebuild packages from SRPMS when you use RedHat 9.1 or higher
            or higher.</p>
          </li>
        </ul> 
-->

        <h3><a name="windows">Binary package for
        MS-Windows</a></h3>

        <ul>
          <li><a href="./win/">HTTP</a><br>
          Windows Version does not contain the training
          programs.<br>
          Model files generated under 'i386 Linux' can be used in
          MS-Windows version.</li>
        </ul>
      </li>
    </ul>

    <h2><a name="install">Installation</a></h2>

    <ul>
      <li>
        Requirements 

        <ul>
          <li>perl 5.00x or higher</li>

          <li>GNU make</li>

          <li>sort, uniq, rm, cat (they are fundamental UNIX
          tools)</li>

          <li><a href=
          "http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/">TinySVM</a></li>

          <li>C++ compiler (gcc 2.95 or higher)</li>
        </ul>
      </li>

      <li>
        How to make 
<pre>
% ./configure 
% make
% make check
% su
# make install
</pre>
        You can change default install path by using --prefix
        option of configure script.<br>
        Try --help option for finding out other options.
      </li>
    </ul>

  <h2><a name="usage">Usage</a></h2>

  <h3><a name="format">Training and Test file formats</a></h3> 

        <p>Both the training file and the test file need to be in a
        particular format for <b>YamCha</b> to work properly.
        Generally speaking, training and test file must consist of
        multiple <b>tokens</b>. In addition, a <b>token</b>
        consists of multiple (but fixed-numbers) columns. The
        definition of tokens depends on tasks, however, in 
        most of typical cases, they simply correspond to 
        <b>words</b>. Each token must be represented in one line,
        with the columns separated by white space (spaces or
        tabular characters). A sequence of token becomes a
        <b>sentence</b>. To identify the boundary between
        sentences, just put an empty line (or just put 'EOS').</p>

        <p>You can give as many columns as you like, however the
        number of columns must be fixed through all tokens.
        Furthermore, there are some kinds of "semantics" among the
        columns. For example, 1st column is 'word', second column
        is 'POS tag' third column is 'sub-category of POS' and so
        on.</p>

        <p>The last column represents a true answer tag which is
        going to be trained by SVMs.</p>

        <p>Here's an example of such a file: (data for CoNLL shared
        task)</p>
<pre>
He        PRP  B-NP
reckons   VBZ  B-VP
the       DT   B-NP
current   JJ   I-NP
account   NN   I-NP
deficit   NN   I-NP
will      MD   B-VP
narrow    VB   I-VP
to        TO   B-PP
only      RB   B-NP
#         #    I-NP
1.8       CD   I-NP
billion   CD   I-NP
in        IN   B-PP
September NNP  B-NP
.         .    O

He        PRP  B-NP
reckons   VBZ  B-VP
..
</pre>

        <p>There are 3 columns for each token.</p>

        <ul>
          <li>The word itself (e.g. reckons);</li>

          <li>part-of-speech associated with the word (e.g.
          VBZ);</li>

          <li>Chunk(Answer) tag represented in IOB2 format;</li>
        </ul>
        <br>
        <br>
         

        <p>The following data is invalid, since the number of
        columns of second and third are 2. (They have no POS
        column.) The number of columns should be fixed.</p>
<pre>
He        PRP  B-NP
reckons   B-VP
the       B-NP
current   JJ   I-NP
account   NN   I-NP
..
</pre>

        <p>Here is an example of English POS-tagging.<br>
        There are total 12 columns; 1: word, 2: contains
        number(Y/N), 3: capitalized(Y/N), 4:contains symbol
        (Y/N)<br>
        5..8 (prefixes from 1 to 4) 9..12 (suffixes from 1 to
        4).<br>
        If there is no entry in a column, dummy field ("__nil__")
        is assigned.</p>
<pre>
Rockwell N Y N R Ro Roc Rock l ll ell well NNP
International N Y N I In Int Inte l al nal onal NNP
Corp. N Y N C Co Cor Corp . p. rp. orp. NNP
's N N N ' 's __nil__ __nil__ s 's __nil__ __nil__ POS
Tulsa N Y N T Tu Tul Tuls a sa lsa ulsa NNP
unit N N N u un uni unit t it nit unit NN
said N N N s sa sai said d id aid said VBD
..
</pre>

  <h3><a name="training">Training and Testing</a></h3> 

        <p>The first step in using the <b>YamCha</b> is to create
        training and test files. Here, I take the <a href=
        "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">
        Base NP Chunking</a> task as a case study.</p>

        <p>Assume a data set like <a href="train.data">this</a>.
        First column represents a word. Second column represents a
        POS tag associated with the word. Third column is true
        answer tag associated with the word (I,O or B). The chunks
        are represented using IOB2 model. The sentences are
        presumed to be separated by one blank line.</p>

        <p>First of all, run <b>yamcha-config</b> with
        <b>--libexecdir</b> option. The location of Makefile which
        is used for training is output. Please copy the Makefile to
        the local working directory.</p>
<pre>
% yamcha-config --libexecdir
/usr/local/libexec/yamcha
% cp /usr/local/libexec/yamcha/Makefile .
</pre>

        <p>There are two mandatory parameters for training.</p>

        <ul>
          <li><b>CORPUS</b>: The location of file which is written
          in the <a href="#format">training/test</a> format.</li>

          <li><b>MODEL</b>: Prefix name of model file(s)</li>
        </ul>

        <p>Here is an example in which <b>CORPUS</b> is set as
        'train.data' and <b>MODEL</b> is set as 'case_study'.</p>
<pre>
% make CORPUS=train.data MODEL=case_study train
/usr/bin/yamcha  -F'F:-2..2:0.. T:-2..-1' &lt; train.data &gt; case_study.data
perl -w /usr/local/libexec/yamcha/mkparam   case_study &lt; case_study.data
perl -w /usr/local/libexec/yamcha/mksvmdata case_study
.. omit
</pre>

        <p>After training, the following files are generated.</p>
<pre>
% ls case_study.*
case_study.log           : log of training
case_study.model         : model file (binary, architecture dependent)
case_study.txtmodel.gz   : model file (text, architecture independent)
case_study.se            : support examples
case_study.svmdata       : training data for SVMs
</pre>

        <p>OK, let's parse this <a href="train.data">test data</a>
        using above generated model file (case_study.model). You
        simply use the command:</p>
<pre>
% yamcha -m case_study.model &lt; test.data 
Rockwell        NNP     B       B
International   NNP     I       I
Corp.   NNP     I       I
's      POS     B       B
Tulsa   NNP     I       I
unit    NN      I       I
said    VBD     O       O
...
</pre>

        <p>The last column is given (estimated) tag. If the 3rd
        column is true answer tag , you can evaluate the accuracy
        by simply seeing the difference between the 3rd and 4th
        columns.</p>

  <h3><a name="tuning">Parameter Tuning</a> </h3>

        <ul>
          <li>
            <b>Parsing Direction</b>           </li>

            <p><b>DIRECTION</b> is used to change the parsing
            direction. The default setting is forward parsing mode
            (LEFT to RIGHT). If "-B" is specified, backward parsing
            mode (RIGHT to LEFT) is used. Please see my paper for
            more detail about the parsing direction.</p>
<pre>
% make CORPUS=train.data MODEL=case_study DIRECTION="-B" train   
</pre>

          <li>
            <b>Re-definition of features (changing window-size)</b>          </li>
            
            <p><b>FEATURE</b> is used to change the feature sets
            (window-size) for chunking.<br>
             The default setting is <b>"F:-2..2:0..
            T:-2..-1"</b>.</p>

            <p><b>"F:-2..2:0.. T:-2..-1"</b> implies that contexts
            in the blue box are used as feature sets to identify
            the tag in the red box.</p>

            <p><img src="./feature.png" alt="features"></p>

            <p>More specifically, the contexts in the blue box can
            be divided into two parts -- green box (static feature
            F:) and light-blue box (dynamic feature T:).<br>
             F: and T: should be written in the following
            format:</p>
<pre>
F:[beginning pos. of token]..[end pos. of token]:[beginning pos. of column]..[end pos. of column]
T:[beginning pos. of tag]..[end pos. of tag]
</pre>

            <p><b>Static Features F:</b><br>
             In this figure, the tokens at -2, -1, 0, 1, and 2
            position are used as features. (green box).<br>
             It means that [beginning positing of token] is
            <b>-2</b> and [end position of token] is <b>+2</b>.<br>
             In addition, this figure shows that 0-th and 1-st
            columns in these tokens are taken as features.<br>
             It means that [beginning position of column] is
            <b>0</b> and [the end position of column] is
            <b>1</b>.<br>
             You can omit the [end position of column]. If omitted,
            the last column is set as [end position of column].<br>
             Note that column for answer tag is not regarded as
            [end position of column].<br>
             By taking tokens as well as columns, final expression
            of static feature becomes <b>"F:-2..2:0..1"</b>.<br>
             In this case, you can use <b>"F:-2..2:0.."</b> which
            means same as <b>"F:-2..2:0..1"</b>.</p>

            <p><b>Dynamic Features T:</b><br>
             Dynamic features are decided dynamically during the
            tagging of chunk labels.<br>
             In this figure, the tags at -2 and -1 position are
            used as features. (light-blue box)<br>
             It means that [beginning positing of tag] is <b>-2</b>
            and [end position of tag] is <b>-1</b>.<br>
             Note that [end potion of tag] must smaller than -1,
            since the right-side tags (0,+1,+2,+3...)<br>
             have not been identified yet and cannot be used as
            features.</p>

            <p>You can use the expression F: and T: repeatably. All
            duplicate entries are deleted.</p>

            <p>Here are more complicated examples.</p>

            <table border="1">
              <tr>
                <td align="center" width="350"><img src=
                "./feature2.png"><br>
                 <b>F:-3..3:0.. T:-3..-1</b></td>

                <td align="center" width="350"><img src=
                "./feature3.png"><br>
                 <b>F:-2..2:1..1 F:0..0:0..1 T:-1..-1</b></td>
              </tr>

              <tr>
                <td align="center" width="350"><img src=
                "./feature4.png"><br>
                 <b>F:-3..-2:0.. F:0..0:0.. F:2..3:0..
                T:-3..-2</b></td>

                <td align="center" width="350"><img src=
                "./feature5.png"><br>
                 <b>F:-3..-2:1..1 F:-1..0:0..0 F:2..3:1..1
                T:-3..-1</b></td>
              </tr>
            </table>

            <p>Here is an example of setting <b>"F:-3..3:0..
            T:-3..-1"</b> to the <b>FEATURE</b> parameter.</p>
<pre>
% make CORPUS=train.data MODEL=case_study FEATURE="F:-3..3:0.. T:-3..-1" train
</pre>

            <p>The expression <b>"-2..2"</b> can be also expressed
            as <b>"-2,-1,0,-1,2"</b>. In addition, if the beginning
            position and end position are same, you can omit the
            end position. Here are some alternative
            expressions:</p>

            <ul>
              <li><b>"F:-2..2:0..0"</b> -&gt;
              <b>"F:-2,-1,0,1,2:0"</b></li>

              <li><b>"F:0..0:0..1"</b> -&gt; <b>"F:0:0,1"</b></li>
            </ul>

            <p>Note that the expression of <b>"-2,0,2"</b> is
            different from <b>"-2..2"</b>.<br>
             ".." represents a range between beginning and end
            position.</p>

	 <li>
	 <b>Call-back function to rewrite features in detail
	 (require C++ knowledge)</b> </li>

            <p>You can define some call-back function which
            re-writes or adds task-dependent specific features. For
            more detail, see example/example.cpp.</p>

          <li>
            <b>Multi-class methods</b>           </li>

            <p><b>MULTI_CLASS</b> is used to change the strategy
            for the multi-class problem. The default setting is
            <b>pair wise</b> method. If "2" is specified, 'one vs
            rest' is used.</p>
<pre>
% make CORPUS=train.data MULTI_CLASS=2 MODEL=case_study
</pre>
          <li>
            <b>Training conditions of SVMs</b>           </li>


            <p><b>SVM_PARAM</b> is used to change the training
            condition of SVMs. Default setting is <b>"-t1 -d2
            -c1"</b>, which means the 2nd degree of polynomial
            kernel and 1 slack variable are used. Note that
            <b>YamCha</b> only supports <b>polynomial</b>
            kernels.</p>

<p>
            Here is an example of using the 3rd degree of polynomial
            kernel: </p>
<pre>
% make CORPUS=train.data MODEL=case_study SVM_PARAM="-t1 -d3 -c1" train
</pre>

            <p>Please use -m SIZE option to increase the memory for
            training if possible. This option drastically reduce
            the computational cost and time.<br>
             Here is an example of assigning 512 Mb memory to the
            SVMs:</p>
<pre>
% make CORPUS=train.data MODEL=case_study SVM_PARAM="-t1 -d2 -c1 -m 512" train
</pre>

          <li> <b>Output format</b> </li>

            <p>The <b>-V</b> option sets verbose mode, where yamcha
            outputs tag and scores of all candidates.<br>
            The meaning of <b>score</b> varies with multi-class
            methods.</p>

            <ul>
              <li>one vs rest: distance from the separating
              hyperplane</li>

              <li>pair wise: summation of distances of this
              class</li>
            </ul>
            <br>
            <br>
             
<pre>
# without -V
% yamcha  -m case_study.model &lt; test.data
Rockwell        NNP     B       B
International   NNP     I       I
Corp.   NNP     I       I
's      POS     B       B
Tulsa   NNP     I       I
unit    NN      I       I
said    VBD     O       O
..

# with -V
% yamcha -V -m case_study.model
Rockwell        NNP     B       B       B/0.630616      I/-0.974367     O/-0.721942
International   NNP     I       I       B/-0.789851     I/0.561522      O/-0.833703
Corp.   NNP     I       I       B/-0.934675     I/0.486497      O/-0.584086
's      POS     B       B       B/0.418284      I/-0.760627     O/-0.794485
Tulsa   NNP     I       I       B/-0.987653     I/1.06272       O/-1.16405
unit    NN      I       I       B/-0.783824     I/0.845213      O/-1.04919
said    VBD     O       O       B/-1.29512      I/-1.02006      O/0.956885
...
</pre>

          <li><b>Sentence boundary marker</b> </li>

            <p>The <b>-e</b> option sets the sentence boundary
            marker. Default setting is empty ("").<br>
            Here is an example of changing the sentence boundary
            marker to "EOS"</p>
<pre>
% yamcha -e EOS -m case_study.model &lt; test.data
</pre>
            <br>

          <li><b>Partial Chunking</b> </li>

            <p>If you know in advance the candidates of answer tags
            by using some 'prior' knowledge, you may want to select
            answer only from these candidates. Here is a concrete
            example. If the 1st token must be B tag and the 2nd
            token must be selected only from B and I, you give yamcha
            the following test data:</p>
<pre>
Rockwell        NNP     B
International   NNP     B      I
</pre>

            <p>Generally speaking, in the partial chunking mode, 
	       candidates are listed instead of last column.<br>
	       In the partial parsing mode, yamcha must be run with -C
            option.</p>
<pre>
% yamcha -C -m case_study.model &lt; test.data
</pre>

            <p>Note that the interpretation of test data varies
	      according to the -C option.</p>

            <ul>
              <li>With -C option: the last (or more) columns are
              interpreted as candidates.</li>

              <li>Without -C option: the last (or more) columns are
              ignored.</li>
            </ul>
        </ul>
  <h3><a name="#fast">Enable Fast Chunking</a></h3>
<p>
        Classification costs of SVMs are much larger than those of other
        algorithms, such as maximum entropy or decision lists. 
	To realize FAST chunking, two algorithms, PKI and PKE, are
	applied in YamCha. PKI and PKE are about 3-12 and 10-300 holds
        faster than the original SVMs respectively. By default, PKI is
        used. To enable PKE, please recompile model files with -e option:
	<pre>
% yamcha-mkmodel -e foo.txtmodel.gz foo.model
% yamcha -m foo.model &lt; ...
</pre>
</p>
<p>
If -e is not given, PKI is employed.
</p>

<p>
PKI and PKE have the following properties:
	<ul>
	 <li>PKI is not an approximation of SVMs. It performs the same results as the original SVMs.
	 <li>PKI uses less disk space compared to PKE.
	 <li>PKE is much faster than PKI.
	 <li>As PKE is an approximation of SVMs, different results will be obtained.
	     The approximation rates can be controlled by the following two parameters.
	     <ul>
	      <li>-n NUM (minimum support): Use features which occur no
		  less than NUM times in support vectors. Default
		  value is 2.  Smaller value gives a better approximation.
	      <li>-s SIGMA (weight threshold): Use features whose
	          weights are between -SIGMA and SIGMA. 
		  Default value is 0.005. Smaller value gives a better approximation.
             </ul>
	</ul>
<p>
	     Here is an example where NUM and SIGMA are set to be 1 and
	     0.0001 respectively.</p>
	     <pre>
% yamcha-mkmodel -e -n 1 -s 0.0001 foo.txtmodel.gz foo.model
</pre>

<p>
Please see our <a
href="http://cl.aist-nara.ac.jp/~taku-ku/publications/acl2003.pdf">paper</a>
for details.</p>


  <h3><a name="others">Other options</a> </h3>
  <p>See <a href="./yamcha.html">here</a>.</p>

  <h2><a name="bib">Bibliography</a></h2>
    <ul>
      <li><b>YamCha</b> itself:</li>
        <ul>
	 <li>Taku Kudo, Yuji Matsumoto (2003)<br>
	     Fast Methods for Kernel-Based Text Analysis, ACL 2003
	 [<a href="http://cl.aist-nara.ac.jp/~taku-ku/publications/acl2003.pdf">PDF</a>]
          <li>Taku Kudo, Yuji Matsumoto (2001)<br>
           Chunking with Support Vector Machines, NAACL 2001 [<a
          href=
          "http://cl.aist-nara.ac.jp/~taku-ku/publications/naacl2001.pdf">
          PDF</a>]</li>

          <li>Taku Kudo, Yuji Matsumoto (2000)<br>
           Use of Support Vector Learning for Chunk Identification,
          CoNLL-2000 [<a href=
          "http://cl.aist-nara.ac.jp/~taku-ku/publications/conll2000.ps">
          PS</a>]</li>
	</ul>
      <li>Publications where <b>YamCha</b> is applied:</li>
         <ul>
          <li>Hiroyasu Yamada, Taku Kudo, Yuji Matsumoto (2002)<br>
           Japanese Named Entity Extraction using Support Vector
          Machine'', Transactions of IPSJ, Vol. 43, No. 1, pages
          44-53, 2002. (in Japanese)</li>

          <li>Tatsumi Yoshida and Kiyonori Ohtake and Kazuhide
          Yamamoto (2002)<br>
           Comparative Experiments of Chinese Analyzers between
          Support Vector Machines and Minimum Connective Costs
          Method, IPSJ SIG NL-150 (in Japanese) [<a href=
          "http://www.slt.atr.co.jp/~kohtake/publishing/SIG-NL-150.pdf">
          PDF</a>]</li>

          <li>Koichi Takeuchi and Nigel Collier (2002)<br>
           Use of support vector machines in extended named entity,
          CoNLL-2002</li>

	  <li>Kadri Hacioglu and Wayne Ward (2003)<br>
	      Target Word Detection and Semantic Role Chunking using
	      Support Vector Machines,
	      HLT-NAACL 2003 Short Parpers</li>

	  <li>Masayuki Asahara and Yuji Matsumoto (2003)<br>
	      Filler and Disfluency Identification Based on Morphological Analysis and Chunking
	      ISCA and IEEE Workshop on Spontaneous Speech Processing
	      and Recognition 2003
	      [<a href="http://chasen.aist-nara.ac.jp/~masayu-a/article/asahara-sspr-2003.pdf">PDF</a>]
	  <li>Masayuki Asahara and Yuji Matsumoto (2003)<br>
	      Japanese Named Entity Extraction with Redundant Morphological Analysis,
	      HLT-NAACL 2003
	      [<a href="http://chasen.aist-nara.ac.jp/~masayu-a/article/asahara-naacl-2003.pdf">PDF</a>]
	  <li>Goh Chooi Ling and Masayuki Asahara and Yuji Matsumoto (2003)<br>
	      Chinese Unknown Word Identification Using Position Tagging and Chunking
	      ACL 2003 Interractive Posters/Demo 
	      [<a
	      href="http://cl.aist-nara.ac.jp/~ling-g/doc/ACL-2003.pdf">PDF</a>]
	  <li>Pradhan, S., Hacioglu, K., Ward, W., Martin, J., Jurafsky, D., (2003)<br>
	      Semantic Role Parsing: Adding Semantic Structure to
	      Unstructured Text, ICDM 2003
	      [<a href="http://oak.colorado.edu/~spradhan/publications/pradhan-icdm-2003.pdf">PDF</a>]
	   <li>Pradhan, S., Sun, H., Ward, W., Martin, J., Jurafsky, D.,  (2003)<br>
	      Parsing Arguments of Nominalizations in English and Chinese,
	      HLT-NAACL 2004
	      [<a href="http://oak.colorado.edu/~spradhan/publications/pradhan-hlt-2004-b.pdf">PDF</a>]
           <li>Pradhan, S., Ward, W., Hacioglu, K., Martin, J.,   Jurafsky, D., (2004)<br>
	      Shallow Semantic Parsing using Support Vector Machines
	      HLT-NAACL 2004
	      [<a href="http://oak.colorado.edu/~spradhan/publications/pradhan-hlt-2004-a.pdf">PDF</a>]
	 </ul>
  </ul>

    <h2><a name="ack">Acknowledgments</a></h2>
      <p>I would like to appreciate all the people that were
      involved in the development of this software: the members in
      <a href="http://cl.aist-nara.ac.jp/">Computational
      Linguistics Laboratory</a> at <a href=
      "http://www.aist-nara.ac.jp">NAIST</a>, and also to
      particular individuals:</p>

      <ul>
        <li><a href=
        "http://www.slt.atr.co.jp/~kohtake/index.html">Kiyonori
        OHTAKE</a> who gives me a number of patches to fix
        bugs.</li>

        <li>Kaoru Yamamoto who reviews this manual.</li>

        <li><a href=
        "http://cl.aist-nara.ac.jp/staff/matsu/home-e.html">Yuji
        MATSUMOTO</a> who is my supervisor.</li>
      </ul>

    <h2><a name="links">Links</a></h2>
    <ul>
      <li><a href=
      "http://lcg-www.uia.ac.be/~erikt/research/np-chunking.html">NP
      chunking</a></li>

      <li><a href=
      "http://lcg-www.uia.ac.be/conll2000/chunking/">chunking</a></li>

      <li><a href="http://chasen.aist-nara.ac.jp/~masayu-a/p/bar/">bar</a>
	  ,model collection for chasen and yamcha</li>

      <li><a href=
      "http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/">TinySVM</a>,
      my developing SVMs-toolkit</li>

      <li><a href=
      "http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/">CaboCha</a>,
      my developing SVMs-based Japanese Dependency Analyzer (in
      Japanese)</li>
    </ul>
    <hr>

    <p>$Id: index.html,v 1.23 2003/01/06 13:11:21 taku-ku Exp
    $;</p>

    <address>
      taku@chasen.org
    </address>
  </body>
</html>

